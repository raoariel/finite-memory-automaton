<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Finite Memory Automaton by raoariel</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Finite Memory Automaton</h1>
        <p>A Finite Memory Automaton for Static and Dynamic Two-Armed Bernoulli Bandit Problems</p>

        <p class="view"><a href="https://github.com/raoariel/finite-memory-automaton">View the Project on GitHub <small>raoariel/finite-memory-automaton</small></a></p>


        <ul>
          <li><a href="https://github.com/raoariel/finite-memory-automaton/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/raoariel/finite-memory-automaton/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/raoariel/finite-memory-automaton">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h5>
<a id="abstract" class="anchor" href="#abstract" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Abstract</h5>

<p>The multi-armed bandit problem is characterized by a repeated decision of selecting an arm amongst a set of arms with unknown payoff probabilities; the payoff probabilities may remain constant or change over time. An agent gains information about the payoff probabilities from past actions and aims to learn an optimal arm selection strategy for maximal payoff. The two-armed Bernoulli bandit (TABB) problem is a special class of multi-armed bandit problems where there are exactly two arms with payoff structures that follow Bernoulli distributions. Existing approaches to the TABB primarily rely on perfect recall of past actions to generate estimates for arm payoff probabilities; it is further assumed that the decision maker knows a priori whether arm payoff probabilities can change. We present a different approach based on finite automata which demonstrates that an agent can learn a low regret strategy without knowing whether arm payoff probabilities are static or dynamic and without having perfect recall of past actions. Roughly speaking, the automaton works by maintaining a relative ranking of arms based on payoff probabilities rather than estimating precise payoff probabilities.</p>

<blockquote>
<p>Full paper available upon request. 
Contact arielrao[at]cmu.edu.</p>
</blockquote>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/raoariel">raoariel</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
